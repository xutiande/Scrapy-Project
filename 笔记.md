1.scrapy-redis源码分析
    1.RedisPipeline管道：对proces_item中的数据进行保存，存入redis中
    2.RFPDupeFilter加密：实现对request对象加密
    3.Scheduler调度器：实现了决定什么时候把request对象加入抓取的队列中，同时把请求过的对象过滤掉 
    4.request的指纹不在集合中
    5.request的dont_filter为true
        1.start_urls中的url地址会入队，因为他们默认是不过滤的
2.分布式爬虫编写流程
    1.普通爬虫
        1.创建项目
        2.明确目标
        3.编写爬虫
        4.保存数据
    2.分布式爬虫（普通爬虫改造）
        1.改造爬虫
            1.导入scrapy_redis类
            2.继承类
            3.注销start_urls & allowed-domains
            4.设置redis_Key获取start_url
            5.设置__init__获取运行的域
        2.改造配置文件修改
            1.copy配置参数
3.普通爬虫
    1.创建爬虫
        1.scrapy startproject ...
        2.scrapy genspider ... jd.com
    2.目标明确
        1.京东
    3.编写代码
        1.修改url
        2.编写爬虫
            1.通过抓包获取jd图书分类的所有子类的url链接地址
            2.对地址进行清洗为正常的url地址，并将地址存入到列表和字典中
            3.针对每个提取处理的url发起请求，并在请求后的页面进行数据提取
        3.针对反爬
            1.开启随机延迟：DOWNLOAD_DELAY
            2.允许延迟：AUTOTHROTTLE_ENABLED
            3.在setting中设置多个请求头参数：USER_AGENT
            4.middlewares文件中创建一个新的类：UserAgentMiddleware
                1.在类中添加随机延迟，并禁用scrapy中的请求头，使用setting中的
            5.middlewares文件中创建一个新的类：
    4.改造为分布式爬虫
        1.导入分布式爬虫类
        2.继承分布式爬虫类
4.原理
    1.scrapy-redis原理是提供一种在分布式环境运行scrapy爬虫的方式，它使用redis作为分布式队列和去重集合、实现了多个爬虫节点之间的任务调度和数据共享.
        1.
    