1.scrapy-redis源码分析
    1.RedisPipeline管道：对proces_item中的数据进行保存，存入redis中
    2.RFPDupeFilter加密：实现对request对象加密
    3.Scheduler调度器：实现了决定什么时候把request对象加入抓取的队列中，同时把请求过的对象过滤掉 
    4.request的指纹不在集合中
    5.request的dont_filter为true
        1.start_urls中的url地址会入队，因为他们默认是不过滤的
2.分布式爬虫编写流程
    1.普通爬虫
        1.创建项目
        2.明确目标
        3.编写爬虫
        4.保存数据
    2.分布式爬虫（普通爬虫改造）
        1.改造爬虫
            1.导入scrapy_redis类
            2.继承类
            3.注销start_urls & allowed-domains
            4.设置redis_Key获取start_url
            5.设置__init__获取运行的域
        2.改造配置文件修改
            1.copy配置参数